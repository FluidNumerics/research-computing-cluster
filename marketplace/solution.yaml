partnerId: my-partner-id
solutionId: Slurm-GCP
partnerInfo:
  name: Fluid Numerics LLC
  description: |
          Fluid Numerics is a high caliber team with backgrounds in High Performance Computing, both application development and system administration, cloud architecting and engineering, software acceleration, GPU programming, and geophysical sciences. Learn more at https://fluidnumerics.com
  url: https://www.fluidnumerics.com
  shortDescription: This is a short partner description, with at most 300 characters
solutionInfo:
  name: Slurm-GCP
  version: 1.0
  description: |
          Slurm-GCP is a scalable and elastic HPC cluster powered by the Slurm job scheduler and Google Cloud Platform. 
  url: https://help.fluidnumerics.com/slurm-gcp
  tagline: A flexible and scalable HPC cluster powered by Slurm and GCP.
  supportInfo: Fluid Numerics offers support for system maintenance, customizations, and integrations with on-premise and other GCP resoures.
  supportUrl: https://help.fluidnumerics.com/support
  eulaUrl: https://help.fluidnumerics.com/slurm-gcp/eula
  documentations:
    - url: https://help.fluidnumerics.com/slurm-gcp/getting-started
      title: Getting Started
      description: Getting started documentation for administrators and users.
      # List of places where this support info should be displayed
      # check marketing_info.proto for options
      destinations:
        - DESTINATION_POST_DEPLOY
        - DESTINATION_CONFIGURATION
  packagedSoftwareGroups:
    - type: SOFTWARE_GROUP_OS
      components:
        - name: CentOS
          version: 7.7
    - components:
        - name: Slurm
          version: 18.04
          url: https://github.com/SchedMD/slurm
        - name: Singularity
          version: 3.2.1 
          url: https://github.com/sylabs/singularity
        - name: Spack
          version: 0.12.1
          url: https://github.com/spack/spack
        - name: CUDA Toolkit
          version: 10.1
          url: https://developer.nvidia.com/cuda-toolkit
        - name: Manage Hyperthreading
          url: https://github.com/WyattGorman/Manage_Hyperthreading.sh
        - name: Environment Modules
          url: https://github.com/cea-hpc/modules

logo:
  raw:
    contentType: PNG
    # Base 64 encoded image
    content: iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mP8z8BQDwAEhQGAhKmMIQAAAABJRU5ErkJggg==
  description: Solution Logo
spec:
  version: 1.0
  # multiVm option is also available
  multiVm:
    tiers:
      - name: network
        title: Network
        images:
          - project: fluid-cluster-ops
            name: slurm-login
            label: slurm-login
        gceStartupScript:
          bashScriptContent: echo SUCCESS > /var/log/startup-log.txt

        firewallRules:
          - protocol : TCP
            port: 22
            allowedSource: PUBLIC
          - protocol : TCP
            port: 0-65535
            allowedSource: DEPLOYMENT
          - protocol : UDP
            port: 0-65535
            allowedSource: DEPLOYMENT
          - protocol : ICMP
            allowedSource: DEPLOYMENT

      - name: login
        title: Slurm Login Node
        images:
          - project: fluid-cluster-ops
            name: slurm-login
            label: slurm-login
        instanceCount:
          defaultValue: 1
          range:
            startValue: 1
            endValue: 10
        machineType:
          # Check http://cloud.google.com/compute/docs/machine-types for all available types
          defaultMachineType:
            gceMachineType: n1-standard-4
          # Minimum CPU and memory constraints for the machine type to run your solution properly
          minimum:
            cpu: 2
            ramGb: 4
        bootDisk:
          # Describes the default and minimum disk size (GB) that user should have to run your solution properly
          diskSize:
            defaultSizeGb: 30
            minSizeGb: 20
          diskType:
            # For more information about disk types: http://cloud.google.com/compute/docs/reference/latest/diskTypes
            # Some types: pd-standard, local-ssd, pd-ssd
            defaultType: pd-standard
          displayLabel: Boot Disk
        gceStartupScript:
          bashScriptContent: echo SUCCESS > /var/log/startup-log.txt

      - name: controller
        title: Slurm Controller Node
        gcpAuthScopes:
          - scope: COMPUTE
        images:
          - project: fluid-cluster-ops
            name: slurm-controller
            label: slurm-controller
        instanceCount:
          defaultValue: 1
          range:
            startValue: 1
            endValue: 1
        machineType:
          # Check http://cloud.google.com/compute/docs/machine-types for all available types
          defaultMachineType:
            gceMachineType: n1-standard-4
          # Minimum CPU and memory constraints for the machine type to run your solution properly
          minimum:
            cpu: 2
            ramGb: 4
        bootDisk:
          # Describes the default and minimum disk size (GB) that user should have to run your solution properly
          diskSize:
            defaultSizeGb: 30
            minSizeGb: 20
          diskType:
            # For more information about disk types: http://cloud.google.com/compute/docs/reference/latest/diskTypes
            # Some types: pd-standard, local-ssd, pd-ssd
            defaultType: pd-standard
          displayLabel: Boot Disk
        gceStartupScript:
          bashScriptContent: echo SUCCESS > /var/log/startup-log.txt

      - name: compute
        title: Slurm Default Compute Partition
        images:
          - project: fluid-cluster-ops
            name: slurm-compute
            label: slurm-compute
        instanceCount:
          defaultValue: 1
          range:
            startValue: 1
            endValue: 10
        machineType:
          # Check http://cloud.google.com/compute/docs/machine-types for all available types
          defaultMachineType:
            gceMachineType: n1-standard-64
        accelerators:
          - types:
              - nvidia-tesla-k80
              - nvidia-tesla-p100
              - nvidia-tesla-v100
              - nvidia-tesla-p4
              - nvidia-tesla-t4
            defaultType: nvidia-tesla-v100
            # Only values of 0, 1, 2, 4, and 8 are supported.
            defaultCount: 0
            minCount: 0
            maxCount: 8
    # Here we can specify extra input fields to be passed to the deployment manager
        bootDisk:
          # Describes the default and minimum disk size (GB) that user should have to run your solution properly
          diskSize:
            defaultSizeGb: 30
            minSizeGb: 20
          diskType:
            # For more information about disk types: http://cloud.google.com/compute/docs/reference/latest/diskTypes
            # Some types: pd-standard, local-ssd, pd-ssd
            defaultType: pd-standard
          displayLabel: Boot Disk
        gceStartupScript:
          bashScriptContent: echo SUCCESS > /var/log/startup-log.txt


    # Post deployment information that will be displayed to the user to actions upon
    postDeploy:
      actionItems:
        - heading: Log on now
          description: "Start using your application by accessing the following URL: https://www.google.com"
          snippet: $ some code snippets can go here, to show some examples
        - heading: Another label
          description: Another description
          showIf:
            # We make this action only show in the post deployment page if a boolean expression is satisfied
            # In this example, only if an input field (see deployInput sections below) in the DM config UI is selected
            booleanDeployInputField:
              name: booleanField1
      # In the post deployment page we can show a table to expose some more details about the deployed solution
      infoRows:
        - label: Property 1
          value: Value for property 1

    deployInput:
      sections:
        - placement: CUSTOM_TOP
          name: networkSettings
          title: Network Settings
          fields:
            - required: true
              name: useExistingNetwork
              title: Use Existing Network
              booleanCheckbox:
                defaultValue: true


        - placement: TIER
          tier: compute
          name: partitionSettings
          title: Compute Partition Setttings
          description: Provide the specifications for the compute nodes in the default partition
          tooltip: This partition will be created during deployment. The cluster-services CLI can be used to add, remove, and modify partitions after deployment.
          # List of input fields that this section has
          fields:
            - required: true
              name: partitionName
              title: Partition Name
              tooltip: The partition name is what you will find when running the `sinfo` command on the slurm cluster.
              stringBox: 
                defaultValue: default
                validation:
                 description: regex ([A-Za-z0-9]{3,15})
                 regex: ([A-Za-z0-9]{3,15})

            - required: true
              name: staticNodeCount
              title: Static Node Count
              tooltip: Static compute nodes are created with the deployment and are not automatically torn down by Slurm.
              integerBox: 
                defaultValue: 
                  value: 0
                validation:
                  description: Set the number of static nodes for the default partition, between 0 and 1000.
                  min: 
                    value: 0
                  max: 
                    value: 1000

            - required: true
              name: maxNodeCount
              title: Max Node Count
              tooltip: Each compute partition can burst up to the Max Node Count (up to 1000 nodes per partition) with ephemeral compute nodes.
              integerBox: 
                defaultValue: 
                  value: 10
                validation:
                  description: Set the max number of compute nodes for the default partition, between 0 and 1000.
                  min: 
                    value: 0
                  max: 
                    value: 1000

            - required: true
              name: maxWallTime
              title: Max Wall Time
              level: 1
              tooltip: The max wall time specifies the maximum allowable wall-clock time for a slurm resource allocation in this partition. The default value is INFINITE. You can set the max wall time to any integer number of seconds greater than 0.
              stringBox: 
                defaultValue: INFINIT
                validation:
                 description: Specify a max wall time. Either INFINITE or any integer number of seconds greater than 0.
                 regex: ([A-Za-z0-9]{3,15})

            - required: true
              name: disableHyperthreading
              title: Disable Hyperthreading
              level: 1
              booleanCheckbox:
                defaultValue: false

            - required: true
              name: preemptibleBursting
              title: Preembtible Bursting
              level: 1
              booleanCheckbox:
                defaultValue: false

            - required: true
              name: computeLabel
              title: Partition Label
              level: 1
              tooltip: This label is applied to all compute instances created within this .
              stringBox: 
                defaultValue: default
                validation:
                 description: regex ([A-Za-z0-9]{3,15})
                 regex: ([A-Za-z0-9]{3,15})

