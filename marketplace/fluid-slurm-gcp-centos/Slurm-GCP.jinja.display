description:
  author:
    title: Fluid Numerics LLC
    descriptionHtml: >
      Fluid Numerics is a high caliber team with backgrounds in High Performance
      Computing, both application development and system administration, cloud
      architecting and engineering, software acceleration, GPU programming, and
      geophysical sciences. Learn more at https://www.fluidnumerics.com
    shortDescription: 'This is a short partner description, with at most 300 characters'
    url: 'https://www.fluidnumerics.com'
  descriptionHtml: >-
    Fluid-Slurm-GCP is a globally scalable, highly customizable HPC cluster
    powered  by the Slurm job scheduler and Google Cloud Platform. The
    deployment the look-and-feel of traditional HPC clusters with the advantages
    of Google Cloud, including heterogeneous and autoscaling compute nodes.
    This solution comes with Spack for package management, pre-installed GCC/10.2
    and OpenMPI/4.0.2, and Singularity to give you a foundation for building your 
    HPC applications on Google Cloud.
  logo: '@media/Slurm-GCP_store.png'
  tagline: A flexible and scalable HPC cluster powered by Slurm and GCP.
  title: Fluid-Slurm-GCP
  url: 'https://help.fluidnumerics.com/slurm-gcp'
  version: 2.6.0
  eulaUrl: 'https://help.fluidnumerics.com/slurm-gcp/eula'
  softwareGroups:
    - type: SOFTWARE_GROUP_OS
      software:
        - title: CentOS
          version: '7.7'
    - software:
        - title: Slurm
          version: '20.02'
          url: 'https://github.com/SchedMD/slurm'
        - title: GCC/GFortran
          version: 10.2
        - title: OpenMPI
          version: 4.0.2
        - title: Singularity
          version: 3.6.2
          url: 'https://github.com/sylabs/singularity'
        - title: ROCm
          version: 3.9.0
          url: 'https://github.com/RadeonOpenCompute/ROCm'
        - title: Spack
          url: 'https://github.com/spack/spack'
        - title: Lustre-Client
          url: 'http://lustre.org/'
        - title: Manage Hyperthreading
          url: 'https://github.com/WyattGorman/Manage_Hyperthreading.sh'
        - title: Environment Modules
          url: 'https://github.com/cea-hpc/modules'
        - title: Nvidia GPU Drivers
          version: '11.1'
  documentations:
    - title: Getting Started
      url: 'https://fluid-slurm-gcp-codelabs.web.app/create-a-hpc-cluster-on-gcp/'
      description: Getting Started Codelab.
      destinations:
        - DESTINATION_CONFIGURATION
    - title: Codelabs
      url: >-
        https://help.fluidnumerics.com/slurm-gcp/codelabs
      description: >-
        A set of codelabs to help you get oriented with fluid-slurm-gcp
      destinations:
        - DESTINATION_POST_DEPLOY
        - DESTINATION_CONFIGURATION
    - title: Using cluster-services
      url: >-
        https://help.fluidnumerics.com/slurm-gcp/documentation/cluster-services-cli
      description: >-
        Learn how to update compute partitions, external mounts, and slurm
        users.
      destinations:
        - DESTINATION_POST_DEPLOY
  support:
    - title: Support
      descriptionHtml: >-
        Fluid Numerics specializes in customizing and integrating boutique
        solutions that fit your HPC and scientific computing needs. Reach out to
        us for support with Slurm-GCP, hybrid cloud computing, HPC software
        profiling, GPU porting, and performance tuning, HPC application
        containerization,  and organization workflow development.
      showSupportId: true
      url: 'https://help.fluidnumerics.com/support'
input:
  properties:
    - name: zone
      title: Zone
      subtext: >-
        GPU availability is limited to certain zones. <a
        href="https://cloud.google.com/compute/docs/gpus">Learn more</a>
    - name: network_network
      title: Network name
      section: NETWORK_TIER
    - name: network_subnetwork
      title: Subnetwork name
      section: NETWORK_TIER
    - name: network_externalIP
      title: External IP
      tooltip: >-
        An external IP address associated with this instance. Selecting "None"
        will result in the instance having no external internet access. <a
        href="https://cloud.google.com/compute/docs/configure-instance-ip-addresses">Learn
        more</a>
      section: NETWORK_TIER
    - name: login_instanceCount
      title: Instance Count
      tooltip: Specify a value between 1 and 10.
      section: LOGIN_TIER
    - name: login_machineType
      title: Machine type
      section: LOGIN_TIER
    - name: login_bootDiskType
      title: Boot Disk type
      section: LOGIN_TIER
    - name: login_bootDiskSizeGb
      title: Boot Disk size in GB
      section: LOGIN_TIER
    - name: controller_machineType
      title: Machine type
      section: CONTROLLER_TIER
    - name: controller_bootDiskType
      title: Boot Disk type
      section: CONTROLLER_TIER
    - name: controller_bootDiskSizeGb
      title: Boot Disk size in GB
      section: CONTROLLER_TIER
    - name: input_partitionName
      title: Partition Name
      tooltip: >-
        The partition name is what you will find when running the `sinfo`
        command on the slurm cluster.
      validation: 'regex pattern: [a-z]([-a-z0-9]*[a-z0-9])'
      section: COMPUTE_TIER
    - name: input_staticNodeCount
      title: Static Node Count
      tooltip: >-
        Static compute nodes are created with the deployment and are not
        automatically torn down by Slurm.
      validation: >-
        Set the number of static nodes for the default partition, between 0 and
        1000.
      section: COMPUTE_TIER
    - name: input_maxNodeCount
      title: Max Node Count
      tooltip: >-
        Each compute partition can burst up to the Max Node Count (up to 1000
        nodes per partition) with ephemeral compute nodes.
      validation: >-
        Set the max number of compute nodes for the default partition, between 0
        and 1000.
      section: COMPUTE_TIER
    - name: compute_machineType
      title: Machine type
      section: COMPUTE_TIER
    - name: compute_acceleratorType
      title: GPUs
      section: COMPUTE_TIER
    - name: compute_bootDiskType
      title: Boot Disk type
      section: COMPUTE_TIER
    - name: compute_bootDiskSizeGb
      title: Boot Disk size in GB
      section: COMPUTE_TIER
    - name: compute_externalIP
      title: External IP
      tooltip: >-
        An external IP address associated with this instance. Selecting "None"
        will result in the instance having no external internet access. <a
        href="https://cloud.google.com/compute/docs/configure-instance-ip-addresses">Learn
        more</a>
      section: COMPUTE_TIER
    - name: input_n_local_ssds
      title: Number of Local SSDs
      level: 1
      tooltip: >-
        Specify the number of local SSDs to attach to compute nodes in this
        partition.  Each local SSD is 375 GB in size. See
        https://cloud.google.com/compute/docs/disks/local-ssd   for more
        information on local SSDs.
      validation: Value must be between 0 and 8 (inclusive).
      section: COMPUTE_TIER
    - name: input_local_ssd_mount_directory
      title: Scratch Directory
      level: 1
      tooltip: 'Specify the path where local SSDs should be mounted. '
      section: COMPUTE_TIER
    - name: input_maxWallTime
      title: Max Wall Time
      level: 1
      tooltip: >-
        The max wall time specifies the maximum allowable wall-clock time for a
        slurm resource allocation in this partition. The default value is
        INFINITE. You can set the max wall time to any integer number of seconds
        greater than 0.
      validation: >-
        Specify a max wall time. Either INFINITE or any integer number of
        seconds greater than 0.
      section: COMPUTE_TIER
    - name: input_disableHyperthreading
      title: Disable Hyperthreading
      level: 1
      section: COMPUTE_TIER
    - name: input_preemptibleBursting
      title: Preembtible Bursting
      level: 1
      section: COMPUTE_TIER
  sections:
    - name: NETWORK_TIER
      title: Network
    - name: LOGIN_TIER
      title: Slurm Login Node
    - name: CONTROLLER_TIER
      title: Slurm Controller Node
    - name: COMPUTE_TIER
      title: Slurm Default Compute Partition
runtime:
  deployingMessage: Deployment can take several minutes to complete.
  applicationTable:
    rows:
      - label: Login Name
        value: '{{ outputs()["login_vmName" + 0] }}'
      - label: Login Address
        value: '{{ outputs()["login_vmExternalIP" + 0] }}'
      - label: Controller Name
        value: '{{ outputs()["controller_vmName" + 0] }}'
      - label: Controller Address
        value: '{{ outputs()["controller_vmExternalIP" + 0] }}'
      - label: Zone
        value: '{{ properties().zone }}'
  primaryButton:
    label: SSH
    type: TYPE_GCE_VM_SSH
    action: '{{ outputs()["login_vmSelfLink" + 0] }}'
  suggestedActions:
    - heading: Add Slurm Users
      description: ssh into the login node and add slurm users
      snippet: 'sudo -i cluster-services add user USERNAME'
annotations:
  autogenSpecType: MULTI_VM
metadata_version: v1
